{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200b2780",
   "metadata": {},
   "source": [
    "# HomeAssistant BERT Training Data generation\n",
    "\n",
    "This notebook is used to generate data to train the BERT model for using sentences in Catalan.\n",
    "\n",
    "It is based in the existing intent definition in catalan in:\n",
    "https://github.com/home-assistant/intents/tree/main/sentences/ca\n",
    "\n",
    "Data from that repository is not to train a BERT system but for using it as a phrase structure to interpret the senteces to generate intents.\n",
    "\n",
    "In this notebook, we will expand those phrases to be able to use them to train a BERT system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec80b70",
   "metadata": {},
   "source": [
    "## Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ce78b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in c:\\users\\rrena\\miniconda3\\lib\\site-packages (6.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\rrena\\miniconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\rrena\\miniconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rrena\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rrena\\miniconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rrena\\miniconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rrena\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyyaml pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30a0ad",
   "metadata": {},
   "source": [
    "## Import required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c25768ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f75fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_expansion_rules(common_file_path):\n",
    "    \"\"\"\n",
    "    Load expansion rules from the _common.yaml file.\n",
    "    \"\"\"\n",
    "    print(f\"Loading expansion rules from {common_file_path}\")\n",
    "    with open(common_file_path, 'r', encoding='utf-8') as f:\n",
    "        content = yaml.safe_load(f)\n",
    "    return content.get('expansion_rules', {})\n",
    "\n",
    "\n",
    "def expand_rules(sentence, expansion_rules):\n",
    "    \"\"\"\n",
    "    Expand rules in the sentence using the provided expansion rules.\n",
    "    \"\"\"\n",
    "    while '<' in sentence and '>' in sentence:\n",
    "        match = re.search(r'<(.*?)>', sentence)\n",
    "        if not match:\n",
    "            break\n",
    "        rule_name = match.group(1)\n",
    "        rule_expansion = expansion_rules.get(rule_name, f\"<{rule_name}>\")\n",
    "        old_sentence = sentence\n",
    "        sentence = sentence.replace(f\"<{rule_name}>\", rule_expansion, 1)\n",
    "        if sentence == old_sentence:\n",
    "            print(f\"Warning: No expansion found for {rule_name}. Keeping original.\")\n",
    "            break\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def expand_blocks(sentence):\n",
    "    \"\"\"\n",
    "    Expand blocks in the sentence between the specified initial and end characters.\n",
    "    \"\"\"\n",
    "    initial_chars = ['(','[']\n",
    "    end_chars = [')',']']\n",
    "    expanded_sentences = []\n",
    "    expanded=False\n",
    "    #if sentence contains any of the initial characters and end characters\n",
    "    if any(char in sentence for char in initial_chars) and any(char in sentence for char in end_chars):\n",
    "        end_char_pos_found= False\n",
    "        initial_char_pos2_found = False\n",
    "        for initial_char_pos in range(len(sentence)):\n",
    "            if sentence[initial_char_pos] in initial_chars:\n",
    "                break;\n",
    "        for end_char_pos in range(initial_char_pos+1, len(sentence)):\n",
    "            if sentence[end_char_pos] in end_chars:\n",
    "                end_char_pos_found = True\n",
    "                break;\n",
    "\n",
    "        for initial_char_pos2 in range(initial_char_pos+1, len(sentence)):\n",
    "            if sentence[initial_char_pos2] in initial_chars:\n",
    "                initial_char_pos2_found = True\n",
    "                break;\n",
    "        \n",
    "        if end_char_pos_found and initial_char_pos2_found and initial_char_pos2 < end_char_pos:\n",
    "            #execute the expansion recursive between the initial2 and end characters                       \n",
    "            generatedsubstrings,expanded = expand_blocks(sentence[initial_char_pos2:end_char_pos+1])\n",
    "            for generatedsubstring in generatedsubstrings:\n",
    "                expanded_sentences.append(sentence[:initial_char_pos2] + generatedsubstring + sentence[end_char_pos+1:])\n",
    "        else:\n",
    "            #expand the sentence between the initial and end characters generate as may sentences as values separeted by |            \n",
    "            options = sentence[initial_char_pos+1:end_char_pos].split('|')            \n",
    "            for option in options:                \n",
    "                expanded_sentences.append(sentence[:initial_char_pos] + option + sentence[end_char_pos+1:])\n",
    "                expanded=True\n",
    "    else:\n",
    "        expanded_sentences = [sentence]\n",
    "    return expanded_sentences, expanded\n",
    "\n",
    "def expand_sentence_blocks(sentence):\n",
    "    \"\"\"\n",
    "    Expand blocks in the sentence using the provided expansion rules.\n",
    "    \"\"\"    \n",
    "    sentences= [sentence]\n",
    "    expanded=True\n",
    "    while expanded:\n",
    "        expanded=False\n",
    "        outsentences = []\n",
    "        for sentence in sentences:\n",
    "            expanded_sentences,expanded_inner=expand_blocks(sentence)\n",
    "            if expanded_inner:\n",
    "                expanded=True\n",
    "            for expanded_sentence in expanded_sentences:\n",
    "                outsentences.append(expanded_sentence)\n",
    "        #remove duplicates\n",
    "        for i in range(len(outsentences)):\n",
    "            for j in range(i+1, len(outsentences)):\n",
    "                if outsentences[i] == outsentences[j]:\n",
    "                    outsentences.pop(j)\n",
    "                    break\n",
    "        sentences = outsentences\n",
    "    return sentences\n",
    "\n",
    "def expand_sentence(sentence, expansion_rules):\n",
    "    \"\"\"\n",
    "    Expand a sentence using the provided expansion rules.\n",
    "    \"\"\"\n",
    "    sentences = [sentence]\n",
    "    outsentences=[]\n",
    "    for sentence in sentences:\n",
    "        outsentences.append(expand_rules(sentence, expansion_rules))\n",
    "\n",
    "    sentences = outsentences\n",
    "    outsentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence_outsentences = expand_sentence_blocks(sentence)\n",
    "        for sentence_outsentence in sentence_outsentences:\n",
    "            outsentences.append(sentence_outsentence)\n",
    "    sentences = outsentences\n",
    "    return sentences\n",
    "\n",
    "def load_sentences_from_yaml(file_path, expansion_rules):\n",
    "    \"\"\"\n",
    "    Load sentences from a YAML file and expand them using the provided expansion rules.\n",
    "    \"\"\"\n",
    "    print(f\"Loading sentences from {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = yaml.safe_load(f)\n",
    "    data = []\n",
    "    # Navigate through the YAML structure\n",
    "    for intent_name, intent_data in content.get('intents', {}).items():\n",
    "        for item in intent_data.get('data', []):\n",
    "            sentences = item.get('sentences', [])\n",
    "            for sentence in sentences:\n",
    "                expanded = expand_sentence(sentence,expansion_rules)\n",
    "                for s in expanded:\n",
    "                    data.append({'sentence': s, 'intent': intent_name})\n",
    "    return data\n",
    "\n",
    "def process_directory(yaml_dir):\n",
    "    all_data = []\n",
    "    # Process general YAML files\n",
    "    common_file_path = os.path.join(yaml_dir, \"_common.yaml\")\n",
    "    expansion_rules = load_expansion_rules(common_file_path)\n",
    "\n",
    "    # Process each YAML file in the directory\n",
    "    for file_name in os.listdir(yaml_dir):\n",
    "        print(file_name)\n",
    "        if file_name.endswith('.yaml') or file_name.endswith('.yml'):\n",
    "            path = os.path.join(yaml_dir, file_name)\n",
    "            sentences=load_sentences_from_yaml(path,expansion_rules)\n",
    "            all_data.extend(sentences)\n",
    "            print(f\"Loaded {len(sentences)} sentences from {file_name}\")\n",
    "    return all_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d90358",
   "metadata": {},
   "source": [
    "## Process directory where intens are present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e1c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expansion rules from .\\from_ha_intents\\sentences\\ca\\_common.yaml\n",
      "assist_satellite_HassBroadcast.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\assist_satellite_HassBroadcast.yaml\n",
      "Loaded 6 sentences from assist_satellite_HassBroadcast.yaml\n",
      "climate_HassClimateGetTemperature.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\climate_HassClimateGetTemperature.yaml\n",
      "Loaded 248 sentences from climate_HassClimateGetTemperature.yaml\n",
      "climate_HassClimateSetTemperature.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\climate_HassClimateSetTemperature.yaml\n",
      "Loaded 1098 sentences from climate_HassClimateSetTemperature.yaml\n",
      "climate_HassTurnOff.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\climate_HassTurnOff.yaml\n",
      "Loaded 14618 sentences from climate_HassTurnOff.yaml\n",
      "climate_HassTurnOn.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\climate_HassTurnOn.yaml\n",
      "Loaded 30948 sentences from climate_HassTurnOn.yaml\n",
      "cover_HassGetState.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\cover_HassGetState.yaml\n",
      "Loaded 0 sentences from cover_HassGetState.yaml\n",
      "cover_HassSetPosition.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\cover_HassSetPosition.yaml\n",
      "Loaded 30422 sentences from cover_HassSetPosition.yaml\n",
      "cover_HassTurnOff.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\cover_HassTurnOff.yaml\n",
      "Loaded 2450 sentences from cover_HassTurnOff.yaml\n",
      "cover_HassTurnOn.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\cover_HassTurnOn.yaml\n",
      "Loaded 4410 sentences from cover_HassTurnOn.yaml\n",
      "fan_HassTurnOff.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\fan_HassTurnOff.yaml\n",
      "Loaded 2880 sentences from fan_HassTurnOff.yaml\n",
      "fan_HassTurnOn.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\fan_HassTurnOn.yaml\n",
      "Loaded 2160 sentences from fan_HassTurnOn.yaml\n",
      "homeassistant_HassCancelAllTimers.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\homeassistant_HassCancelAllTimers.yaml\n",
      "Loaded 1240 sentences from homeassistant_HassCancelAllTimers.yaml\n",
      "homeassistant_HassCancelTimer.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\homeassistant_HassCancelTimer.yaml\n",
      "Loaded 640 sentences from homeassistant_HassCancelTimer.yaml\n",
      "homeassistant_HassDecreaseTimer.yaml_DONOTUSE\n",
      "homeassistant_HassGetCurrentDate.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\homeassistant_HassGetCurrentDate.yaml\n",
      "Loaded 110 sentences from homeassistant_HassGetCurrentDate.yaml\n",
      "homeassistant_HassGetCurrentTime.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\homeassistant_HassGetCurrentTime.yaml\n",
      "Loaded 59 sentences from homeassistant_HassGetCurrentTime.yaml\n",
      "homeassistant_HassGetState.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\homeassistant_HassGetState.yaml\n",
      "Loaded 1056 sentences from homeassistant_HassGetState.yaml\n",
      "homeassistant_HassIncreaseTimer.yaml_DONOTUSE\n",
      "homeassistant_HassNevermind.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\homeassistant_HassNevermind.yaml\n",
      "Loaded 10 sentences from homeassistant_HassNevermind.yaml\n",
      "homeassistant_HassPauseTimer.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\homeassistant_HassPauseTimer.yaml\n",
      "Loaded 76 sentences from homeassistant_HassPauseTimer.yaml\n",
      "homeassistant_HassRespond.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\homeassistant_HassRespond.yaml\n",
      "Loaded 35 sentences from homeassistant_HassRespond.yaml\n",
      "homeassistant_HassStartTimer.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\homeassistant_HassStartTimer.yaml\n",
      "Warning: No expansion found for of. Keeping original.\n",
      "Warning: No expansion found for of. Keeping original.\n",
      "Warning: No expansion found for of. Keeping original.\n",
      "Warning: No expansion found for of. Keeping original.\n",
      "Loaded 1748 sentences from homeassistant_HassStartTimer.yaml\n",
      "homeassistant_HassTimerStatus.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\homeassistant_HassTimerStatus.yaml\n",
      "Warning: No expansion found for quant_queda. Keeping original.\n",
      "Warning: No expansion found for quant_queda. Keeping original.\n",
      "Warning: No expansion found for quant_queda. Keeping original.\n",
      "Warning: No expansion found for dades. Keeping original.\n",
      "Loaded 15 sentences from homeassistant_HassTimerStatus.yaml\n",
      "homeassistant_HassTurnOff.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\homeassistant_HassTurnOff.yaml\n",
      "Loaded 648 sentences from homeassistant_HassTurnOff.yaml\n",
      "homeassistant_HassTurnOn.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\homeassistant_HassTurnOn.yaml\n",
      "Loaded 1136 sentences from homeassistant_HassTurnOn.yaml\n",
      "homeassistant_HassUnpauseTimer.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\homeassistant_HassUnpauseTimer.yaml\n",
      "Warning: No expansion found for pronom_singular_temporitzador. Keeping original.\n",
      "Warning: No expansion found for pronom_singular_temporitzador. Keeping original.\n",
      "Warning: No expansion found for pronom_singular_temporitzador. Keeping original.\n",
      "Warning: No expansion found for pronom_singular_temporitzador. Keeping original.\n",
      "Warning: No expansion found for pronom_singular_temporitzador. Keeping original.\n",
      "Loaded 176 sentences from homeassistant_HassUnpauseTimer.yaml\n",
      "light_HassLightSet.yaml_DONOTUSE\n",
      "light_HassTurnOff.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\light_HassTurnOff.yaml\n",
      "Warning: No expansion found for everywhere. Keeping original.\n",
      "Warning: No expansion found for everywhere. Keeping original.\n",
      "Loaded 19972 sentences from light_HassTurnOff.yaml\n",
      "light_HassTurnOn.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\light_HassTurnOn.yaml\n",
      "Warning: No expansion found for everywhere. Keeping original.\n",
      "Warning: No expansion found for everywhere. Keeping original.\n",
      "Loaded 48823 sentences from light_HassTurnOn.yaml\n",
      "lock_HassTurnOff.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\lock_HassTurnOff.yaml\n",
      "Loaded 2640 sentences from lock_HassTurnOff.yaml\n",
      "lock_HassTurnOn.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\lock_HassTurnOn.yaml\n",
      "Loaded 784 sentences from lock_HassTurnOn.yaml\n",
      "media_player_HassMediaNext.yaml_DONOTUSE\n",
      "media_player_HassMediaPause.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\media_player_HassMediaPause.yaml\n",
      "Loaded 60 sentences from media_player_HassMediaPause.yaml\n",
      "media_player_HassMediaPrevious.yaml_DONOTUSE\n",
      "media_player_HassMediaUnpause.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\media_player_HassMediaUnpause.yaml\n",
      "Loaded 480 sentences from media_player_HassMediaUnpause.yaml\n",
      "media_player_HassSetVolume.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\media_player_HassSetVolume.yaml\n",
      "Loaded 11760 sentences from media_player_HassSetVolume.yaml\n",
      "person_HassGetState.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\person_HassGetState.yaml\n",
      "Warning: No expansion found for on. Keeping original.\n",
      "Warning: No expansion found for on. Keeping original.\n",
      "Warning: No expansion found for on. Keeping original.\n",
      "Warning: No expansion found for la_persona. Keeping original.\n",
      "Warning: No expansion found for la_persona. Keeping original.\n",
      "Warning: No expansion found for la_persona. Keeping original.\n",
      "Loaded 16 sentences from person_HassGetState.yaml\n",
      "scene_HassTurnOn.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\scene_HassTurnOn.yaml\n",
      "Loaded 496 sentences from scene_HassTurnOn.yaml\n",
      "script_HassTurnOn.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\script_HassTurnOn.yaml\n",
      "Loaded 48 sentences from script_HassTurnOn.yaml\n",
      "shopping_list_HassShoppingListAddItem.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\shopping_list_HassShoppingListAddItem.yaml\n",
      "Warning: No expansion found for add. Keeping original.\n",
      "Warning: No expansion found for add. Keeping original.\n",
      "Loaded 4 sentences from shopping_list_HassShoppingListAddItem.yaml\n",
      "todo_HassListAddItem.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\todo_HassListAddItem.yaml\n",
      "Warning: No expansion found for add. Keeping original.\n",
      "Warning: No expansion found for add. Keeping original.\n",
      "Warning: No expansion found for add. Keeping original.\n",
      "Warning: No expansion found for add. Keeping original.\n",
      "Loaded 20 sentences from todo_HassListAddItem.yaml\n",
      "vacuum_HassVacuumReturnToBase.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\vacuum_HassVacuumReturnToBase.yaml\n",
      "Loaded 48 sentences from vacuum_HassVacuumReturnToBase.yaml\n",
      "vacuum_HassVacuumStart.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\vacuum_HassVacuumStart.yaml\n",
      "Loaded 248 sentences from vacuum_HassVacuumStart.yaml\n",
      "valve_HassSetPosition.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\valve_HassSetPosition.yaml\n",
      "Loaded 156 sentences from valve_HassSetPosition.yaml\n",
      "weather_HassGetWeather.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\weather_HassGetWeather.yaml\n",
      "Warning: No expansion found for al_lloc. Keeping original.\n",
      "Warning: No expansion found for temps. Keeping original.\n",
      "Warning: No expansion found for temps. Keeping original.\n",
      "Warning: No expansion found for al_lloc. Keeping original.\n",
      "Loaded 20 sentences from weather_HassGetWeather.yaml\n",
      "_common.yaml\n",
      "Loading sentences from .\\from_ha_intents\\sentences\\ca\\_common.yaml\n",
      "Loaded 0 sentences from _common.yaml\n",
      "Dataset generat amb 181764 frases i desat a: hass_intents_ca.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    yaml_directory = r\".\\from_ha_intents\\sentences\\ca\"\n",
    "    #yaml_directory = r\".\\test_ca\"\n",
    "    output_csv = \"hass_intents_ca.csv\"\n",
    "\n",
    "    data = process_directory(yaml_directory)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Dataset generat amb {len(df)} frases i desat a: {output_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
