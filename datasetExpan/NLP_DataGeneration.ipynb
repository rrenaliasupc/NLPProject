{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200b2780",
   "metadata": {},
   "source": [
    "# HomeAssistant BERT Training Data generation\n",
    "\n",
    "This notebook is used to generate data to train the BERT model for using sentences in Catalan.\n",
    "\n",
    "It is based in the existing intent definition in catalan in:\n",
    "https://github.com/home-assistant/intents/tree/main/sentences/ca\n",
    "\n",
    "Data from that repository is not to train a BERT system but for using it as a phrase structure to interpret the senteces to generate intents.\n",
    "\n",
    "In this notebook, we will expand those phrases to be able to use them to train a BERT system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec80b70",
   "metadata": {},
   "source": [
    "## Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17ce78b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in c:\\users\\rrena\\miniconda3\\lib\\site-packages (6.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\rrena\\miniconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\rrena\\miniconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rrena\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rrena\\miniconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rrena\\miniconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rrena\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyyaml pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30a0ad",
   "metadata": {},
   "source": [
    "## Import required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c25768ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71f75fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expansion rules from .\\test_ca\\_common.yaml\n",
      "test1.yaml\n",
      "Expanding rule: area -> [<preposicio_singular>]{area}\n",
      "Expanding rule: preposicio_singular -> <preposicio_base_singular> [<pronom_singular>]\n",
      "Expanding rule: preposicio_base_singular -> (en|de|del|a|al)\n",
      "Expanding rule: pronom_singular -> ((el|la|es|sa) |l'|s')\n",
      "_common.yaml\n",
      "Dataset generat amb 1 frases i desat a: hass_intents_ca.csv\n"
     ]
    }
   ],
   "source": [
    "def load_expansion_rules(common_file_path):\n",
    "    \"\"\"\n",
    "    Load expansion rules from the _common.yaml file.\n",
    "    \"\"\"\n",
    "    print(f\"Loading expansion rules from {common_file_path}\")\n",
    "    with open(common_file_path, 'r', encoding='utf-8') as f:\n",
    "        content = yaml.safe_load(f)\n",
    "    return content.get('expansion_rules', {})\n",
    "\n",
    "def expand_rules(sentence, expansion_rules):\n",
    "    \"\"\"\n",
    "    Expand rules in the sentence using the provided expansion rules.\n",
    "    \"\"\"\n",
    "    while '<' in sentence and '>' in sentence:\n",
    "        match = re.search(r'<(.*?)>', sentence)\n",
    "        if not match:\n",
    "            break\n",
    "        rule_name = match.group(1)\n",
    "        rule_expansion = expansion_rules.get(rule_name, f\"<{rule_name}>\")\n",
    "        print(f\"Expanding rule: {rule_name} -> {rule_expansion}\")\n",
    "        old_sentence = sentence\n",
    "        sentence = sentence.replace(f\"<{rule_name}>\", rule_expansion, 1)\n",
    "        if sentence == old_sentence:\n",
    "            print(f\"Warning: No expansion found for {rule_name}. Keeping original.\")\n",
    "            break\n",
    "    return sentence\n",
    "\n",
    "def expand_blocks(sentence, initial_char, end_char):\n",
    "    \"\"\"\n",
    "    Expand blocks in the sentence between the specified initial and end characters.\n",
    "    \"\"\"\n",
    "    if initial_char in sentence and end_char in sentence:\n",
    "        parts = re.split(r'('+initial_char+r'.*?'+end_char+')', sentence)\n",
    "        expanded_sentences = []\n",
    "        \n",
    "        for part in parts:\n",
    "            if part.startswith(initial_char) and part.endswith(end_char):\n",
    "                options = part[1:-1].split('|')\n",
    "                if not expanded_sentences:\n",
    "                    expanded_sentences = options\n",
    "                else:\n",
    "                    expanded_sentences = [\n",
    "                        f\"{prev}{opt}\" for prev in expanded_sentences for opt in options\n",
    "                    ]\n",
    "            else:\n",
    "                if not expanded_sentences:\n",
    "                    expanded_sentences = [part]\n",
    "                else:\n",
    "                    expanded_sentences = [f\"{prev}{part}\" for prev in expanded_sentences]\n",
    "    else:\n",
    "        expanded_sentences = [sentence]\n",
    "    return expanded_sentences\n",
    "\n",
    "def expand_sentence(sentence, expansion_rules):\n",
    "    sentences = [sentence]\n",
    "    outsentences=[]\n",
    "    for sentence in sentences:\n",
    "        outsentences.append(expand_rules(sentence, expansion_rules))\n",
    "\n",
    "    # sentences = outsentences\n",
    "    # outsentences = []\n",
    "    # for sentence in sentences:\n",
    "    #     outsentences.append(expand_blocks(sentence, '(', ')'))\n",
    "\n",
    "    # sentences = outsentences\n",
    "    # outsentences = []\n",
    "    # for sentence in sentences:\n",
    "    #     outsentences.append(expand_blocks(sentence, '[', ']'))\n",
    "\n",
    "    return outsentences\n",
    "\n",
    "\n",
    "def expand_sentence_x(sentence, expansion_rules):\n",
    "    \"\"\"\n",
    "    Expand phrases between [], (), and <> and maintain entities {name}.\n",
    "    Handles nested expandable blocks in a single sentence.\n",
    "    \"\"\"\n",
    "    # Primer, expandim els blocs entre parèntesis com un nivell superior\n",
    "    if '(' in sentence and ')' in sentence:\n",
    "        parts = re.split(r'(\\(.*?\\))', sentence)\n",
    "        expanded_sentences = []\n",
    "        \n",
    "        for part in parts:\n",
    "            if part.startswith('(') and part.endswith(')'):\n",
    "                options = part[1:-1].split('|')\n",
    "                if not expanded_sentences:\n",
    "                    expanded_sentences = options\n",
    "                else:\n",
    "                    expanded_sentences = [\n",
    "                        f\"{prev}{opt}\" for prev in expanded_sentences for opt in options\n",
    "                    ]\n",
    "            else:\n",
    "                if not expanded_sentences:\n",
    "                    expanded_sentences = [part]\n",
    "                else:\n",
    "                    expanded_sentences = [f\"{prev}{part}\" for prev in expanded_sentences]\n",
    "    else:\n",
    "        expanded_sentences = [sentence]\n",
    "\n",
    "    # Ara, expandim els blocs entre claudàtors dins de cada frase generada\n",
    "    final_sentences = []\n",
    "    for expanded in expanded_sentences:\n",
    "        parts = re.split(r'(\\[.*?\\])', expanded)\n",
    "        tokens = []\n",
    "\n",
    "        for part in parts:\n",
    "            if part.startswith('[') and part.endswith(']'):\n",
    "                options = part[1:-1].split('|')\n",
    "                tokens.append(options)\n",
    "            else:\n",
    "                tokens.append([part])\n",
    "\n",
    "        combinations = list(itertools.product(*tokens))\n",
    "        final_sentences.extend([''.join(combo).strip() for combo in combinations])\n",
    "\n",
    "    #return final_sentences\n",
    "\n",
    "    # Finalment, expandim els blocs entre <rule> utilitzant les expansion_rules\n",
    "    fully_expanded_sentences = []\n",
    "    for sentence in final_sentences:\n",
    "        while '<' in sentence and '>' in sentence:\n",
    "            match = re.search(r'<(.*?)>', sentence)\n",
    "            if not match:\n",
    "                break\n",
    "            rule_name = match.group(1)\n",
    "            rule_expansion = expansion_rules.get(rule_name, f\"<{rule_name}>\")\n",
    "            print(f\"Expanding rule: {rule_name} -> {rule_expansion}\")\n",
    "            old_sentence = sentence\n",
    "            sentence = sentence.replace(f\"<{rule_name}>\", rule_expansion, 1)\n",
    "            if sentence == old_sentence:\n",
    "                print(f\"Warning: No expansion found for {rule_name}. Keeping original.\")\n",
    "                break\n",
    "        fully_expanded_sentences.append(sentence)\n",
    "\n",
    "    return fully_expanded_sentences\n",
    "\n",
    "def load_sentences_from_yaml(file_path, expansion_rules):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = yaml.safe_load(f)\n",
    "\n",
    "    data = []\n",
    "    # Navigate through the YAML structure\n",
    "    for intent_name, intent_data in content.get('intents', {}).items():\n",
    "        for item in intent_data.get('data', []):\n",
    "            sentences = item.get('sentences', [])\n",
    "            for sentence in sentences:\n",
    "                expanded = expand_sentence(sentence,expansion_rules)\n",
    "                for s in expanded:\n",
    "                    data.append({'sentence': s, 'intent': intent_name})\n",
    "    return data\n",
    "\n",
    "def process_directory(yaml_dir):\n",
    "    all_data = []\n",
    "    # Process general YAML files\n",
    "    common_file_path = os.path.join(yaml_dir, \"_common.yaml\")\n",
    "    expansion_rules = load_expansion_rules(common_file_path)\n",
    "\n",
    "    # Process each YAML file in the directory\n",
    "    for file_name in os.listdir(yaml_dir):\n",
    "        print(file_name)\n",
    "        if file_name.endswith('.yaml') or file_name.endswith('.yml'):\n",
    "            path = os.path.join(yaml_dir, file_name)\n",
    "            all_data.extend(load_sentences_from_yaml(path,expansion_rules))\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #yaml_directory = r\".\\intents\\sentences\\ca\"\n",
    "    yaml_directory = r\".\\test_ca\"\n",
    "    output_csv = \"hass_intents_ca.csv\"\n",
    "\n",
    "    data = process_directory(yaml_directory)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Dataset generat amb {len(df)} frases i desat a: {output_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
