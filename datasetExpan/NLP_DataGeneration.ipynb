{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200b2780",
   "metadata": {},
   "source": [
    "# HomeAssistant BERT Training Data generation\n",
    "\n",
    "This notebook is used to generate data to train the BERT model for using sentences in Catalan.\n",
    "\n",
    "It is based in the existing intent definition in catalan in:\n",
    "https://github.com/home-assistant/intents/tree/main/sentences/ca\n",
    "\n",
    "Data from that repository is not to train a BERT system but for using it as a phrase structure to interpret the senteces to generate intents.\n",
    "\n",
    "In this notebook, we will expand those phrases to be able to use them to train a BERT system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec80b70",
   "metadata": {},
   "source": [
    "## Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17ce78b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in c:\\users\\rrena\\miniconda3\\lib\\site-packages (6.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\rrena\\miniconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\rrena\\miniconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rrena\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rrena\\miniconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rrena\\miniconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rrena\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyyaml pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30a0ad",
   "metadata": {},
   "source": [
    "## Import required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c25768ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "71f75fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expansion rules from .\\test_ca\\_common.yaml\n",
      "test1.yaml\n",
      "sentences: ['Inici frase [[Hola|Hi]|Adeu] fi frase']\n",
      "expanded_sentences2: ['Hola', 'Hi']\n",
      "expanded_sentences2: ['Inici frase [Hola|Adeu] fi frase', 'Inici frase [Hi|Adeu] fi frase']\n",
      "outsentences: ['Inici frase [Hola|Adeu] fi frase', 'Inici frase [Hi|Adeu] fi frase']\n",
      "sentences: ['Inici frase [Hola|Adeu] fi frase', 'Inici frase [Hi|Adeu] fi frase']\n",
      "expanded_sentences2: ['Inici frase Hola fi frase', 'Inici frase Adeu fi frase']\n",
      "outsentences: ['Inici frase Hola fi frase', 'Inici frase Adeu fi frase']\n",
      "expanded_sentences2: ['Inici frase Hi fi frase', 'Inici frase Adeu fi frase']\n",
      "outsentences: ['Inici frase Hola fi frase', 'Inici frase Adeu fi frase', 'Inici frase Hi fi frase', 'Inici frase Adeu fi frase']\n",
      "sentences: ['Inici frase Hola fi frase', 'Inici frase Adeu fi frase', 'Inici frase Hi fi frase']\n",
      "expanded_sentences2: ['Inici frase Hola fi frase']\n",
      "outsentences: ['Inici frase Hola fi frase']\n",
      "expanded_sentences2: ['Inici frase Adeu fi frase']\n",
      "outsentences: ['Inici frase Hola fi frase', 'Inici frase Adeu fi frase']\n",
      "expanded_sentences2: ['Inici frase Hi fi frase']\n",
      "outsentences: ['Inici frase Hola fi frase', 'Inici frase Adeu fi frase', 'Inici frase Hi fi frase']\n",
      "_common.yaml\n",
      "Dataset generat amb 3 frases i desat a: hass_intents_ca.csv\n"
     ]
    }
   ],
   "source": [
    "def load_expansion_rules(common_file_path):\n",
    "    \"\"\"\n",
    "    Load expansion rules from the _common.yaml file.\n",
    "    \"\"\"\n",
    "    print(f\"Loading expansion rules from {common_file_path}\")\n",
    "    with open(common_file_path, 'r', encoding='utf-8') as f:\n",
    "        content = yaml.safe_load(f)\n",
    "    return content.get('expansion_rules', {})\n",
    "\n",
    "def expand_rules(sentence, expansion_rules):\n",
    "    \"\"\"\n",
    "    Expand rules in the sentence using the provided expansion rules.\n",
    "    \"\"\"\n",
    "    while '<' in sentence and '>' in sentence:\n",
    "        match = re.search(r'<(.*?)>', sentence)\n",
    "        if not match:\n",
    "            break\n",
    "        rule_name = match.group(1)\n",
    "        rule_expansion = expansion_rules.get(rule_name, f\"<{rule_name}>\")\n",
    "        print(f\"Expanding rule: {rule_name} -> {rule_expansion}\")\n",
    "        old_sentence = sentence\n",
    "        sentence = sentence.replace(f\"<{rule_name}>\", rule_expansion, 1)\n",
    "        if sentence == old_sentence:\n",
    "            print(f\"Warning: No expansion found for {rule_name}. Keeping original.\")\n",
    "            break\n",
    "    return sentence\n",
    "\n",
    "def expand_sentence(sentence, expansion_rules):\n",
    "    sentences = [sentence]\n",
    "    outsentences=[]\n",
    "    for sentence in sentences:\n",
    "        outsentences.append(expand_rules(sentence, expansion_rules))\n",
    "\n",
    "    sentences = outsentences\n",
    "    \n",
    "    expanded=True\n",
    "    while expanded:\n",
    "        print(\"sentences:\",sentences)\n",
    "        expanded=False\n",
    "        outsentences = []\n",
    "        for sentence in sentences:\n",
    "            expanded_sentences,expanded_inner=expand_blocks(sentence)\n",
    "            if expanded_inner:\n",
    "                expanded=True\n",
    "            for expanded_sentence in expanded_sentences:\n",
    "                outsentences.append(expanded_sentence)\n",
    "            print(\"outsentences:\",outsentences)\n",
    "        #remove duplicates\n",
    "        for i in range(len(outsentences)):\n",
    "            for j in range(i+1, len(outsentences)):\n",
    "                if outsentences[i] == outsentences[j]:\n",
    "                    outsentences.pop(j)\n",
    "                    break\n",
    "        \n",
    "        sentences = outsentences\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def expand_sentence_x(sentence, expansion_rules):\n",
    "    \"\"\"\n",
    "    Expand phrases between [], (), and <> and maintain entities {name}.\n",
    "    Handles nested expandable blocks in a single sentence.\n",
    "    \"\"\"\n",
    "    # Primer, expandim els blocs entre parèntesis com un nivell superior\n",
    "    if '(' in sentence and ')' in sentence:\n",
    "        parts = re.split(r'(\\(.*?\\))', sentence)\n",
    "        expanded_sentences = []\n",
    "        \n",
    "        for part in parts:\n",
    "            if part.startswith('(') and part.endswith(')'):\n",
    "                options = part[1:-1].split('|')\n",
    "                if not expanded_sentences:\n",
    "                    expanded_sentences = options\n",
    "                else:\n",
    "                    expanded_sentences = [\n",
    "                        f\"{prev}{opt}\" for prev in expanded_sentences for opt in options\n",
    "                    ]\n",
    "            else:\n",
    "                if not expanded_sentences:\n",
    "                    expanded_sentences = [part]\n",
    "                else:\n",
    "                    expanded_sentences = [f\"{prev}{part}\" for prev in expanded_sentences]\n",
    "    else:\n",
    "        expanded_sentences = [sentence]\n",
    "\n",
    "    # Ara, expandim els blocs entre claudàtors dins de cada frase generada\n",
    "    final_sentences = []\n",
    "    for expanded in expanded_sentences:\n",
    "        parts = re.split(r'(\\[.*?\\])', expanded)\n",
    "        tokens = []\n",
    "\n",
    "        for part in parts:\n",
    "            if part.startswith('[') and part.endswith(']'):\n",
    "                options = part[1:-1].split('|')\n",
    "                tokens.append(options)\n",
    "            else:\n",
    "                tokens.append([part])\n",
    "\n",
    "        combinations = list(itertools.product(*tokens))\n",
    "        final_sentences.extend([''.join(combo).strip() for combo in combinations])\n",
    "\n",
    "    #return final_sentences\n",
    "\n",
    "    # Finalment, expandim els blocs entre <rule> utilitzant les expansion_rules\n",
    "    fully_expanded_sentences = []\n",
    "    for sentence in final_sentences:\n",
    "        while '<' in sentence and '>' in sentence:\n",
    "            match = re.search(r'<(.*?)>', sentence)\n",
    "            if not match:\n",
    "                break\n",
    "            rule_name = match.group(1)\n",
    "            rule_expansion = expansion_rules.get(rule_name, f\"<{rule_name}>\")\n",
    "            print(f\"Expanding rule: {rule_name} -> {rule_expansion}\")\n",
    "            old_sentence = sentence\n",
    "            sentence = sentence.replace(f\"<{rule_name}>\", rule_expansion, 1)\n",
    "            if sentence == old_sentence:\n",
    "                print(f\"Warning: No expansion found for {rule_name}. Keeping original.\")\n",
    "                break\n",
    "        fully_expanded_sentences.append(sentence)\n",
    "\n",
    "    return fully_expanded_sentences\n",
    "\n",
    "def load_sentences_from_yaml(file_path, expansion_rules):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = yaml.safe_load(f)\n",
    "\n",
    "    data = []\n",
    "    # Navigate through the YAML structure\n",
    "    for intent_name, intent_data in content.get('intents', {}).items():\n",
    "        for item in intent_data.get('data', []):\n",
    "            sentences = item.get('sentences', [])\n",
    "            for sentence in sentences:\n",
    "                expanded = expand_sentence(sentence,expansion_rules)\n",
    "                for s in expanded:\n",
    "                    data.append({'sentence': s, 'intent': intent_name})\n",
    "    return data\n",
    "\n",
    "def process_directory(yaml_dir):\n",
    "    all_data = []\n",
    "    # Process general YAML files\n",
    "    common_file_path = os.path.join(yaml_dir, \"_common.yaml\")\n",
    "    expansion_rules = load_expansion_rules(common_file_path)\n",
    "\n",
    "    # Process each YAML file in the directory\n",
    "    for file_name in os.listdir(yaml_dir):\n",
    "        print(file_name)\n",
    "        if file_name.endswith('.yaml') or file_name.endswith('.yml'):\n",
    "            path = os.path.join(yaml_dir, file_name)\n",
    "            all_data.extend(load_sentences_from_yaml(path,expansion_rules))\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def expand_blocks(sentence):\n",
    "    \"\"\"\n",
    "    Expand blocks in the sentence between the specified initial and end characters.\n",
    "    \"\"\"\n",
    "    initial_chars = ['(','[']\n",
    "    end_chars = [')',']']\n",
    "    #print(sentence)\n",
    "    expanded_sentences = []\n",
    "    expanded=False\n",
    "    #if sentence contains any of the initial characters and end characters\n",
    "    if any(char in sentence for char in initial_chars) and any(char in sentence for char in end_chars):\n",
    "        end_char_pos_found= False\n",
    "        initial_char_pos2_found = False\n",
    "        for initial_char_pos in range(len(sentence)):\n",
    "            if sentence[initial_char_pos] in initial_chars:\n",
    "                break;\n",
    "        for end_char_pos in range(initial_char_pos+1, len(sentence)):\n",
    "            if sentence[end_char_pos] in end_chars:\n",
    "                end_char_pos_found = True\n",
    "                break;\n",
    "\n",
    "        for initial_char_pos2 in range(initial_char_pos+1, len(sentence)):\n",
    "            if sentence[initial_char_pos2] in initial_chars:\n",
    "                initial_char_pos2_found = True\n",
    "                break;\n",
    "        \n",
    "        #print(\"initial_char_pos:\",initial_char_pos)\n",
    "        #print(\"end_char_pos:\",end_char_pos)\n",
    "        #print(\"initial_char_pos2:\",initial_char_pos2)\n",
    "\n",
    "        if end_char_pos_found and initial_char_pos2_found and initial_char_pos2 < end_char_pos:\n",
    "            #execute the expansion recursive between the initial2 and end characters                       \n",
    "            generatedsubstrings,expanded = expand_blocks(sentence[initial_char_pos2:end_char_pos+1])\n",
    "            #print(\"generatedsubstrings:\",generatedsubstrings)\n",
    "            for generatedsubstring in generatedsubstrings:\n",
    "                #print(\"generatedsubstring:\",generatedsubstring)\n",
    "                expanded_sentences.append(sentence[:initial_char_pos2] + generatedsubstring + sentence[end_char_pos+1:])\n",
    "        else:\n",
    "            #expand the sentence between the initial and end characters generate as may sentences as values separeted by |\n",
    "            \n",
    "            options = sentence[initial_char_pos+1:end_char_pos].split('|')\n",
    "            #print(\"options:\",options)\n",
    "            for option in options:\n",
    "                #print(\"option:\",option)\n",
    "                expanded_sentences.append(sentence[:initial_char_pos] + option + sentence[end_char_pos+1:])\n",
    "                expanded=True\n",
    "\n",
    "            #print(\"expanded_sentences1:\",expanded_sentences)\n",
    "        \n",
    "    else:\n",
    "        expanded_sentences = [sentence]\n",
    "    print(\"expanded_sentences2:\",expanded_sentences)\n",
    "    #print(\"expanded:\",expanded)\n",
    "    \n",
    "    return expanded_sentences, expanded\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #yaml_directory = r\".\\intents\\sentences\\ca\"\n",
    "    yaml_directory = r\".\\test_ca\"\n",
    "    output_csv = \"hass_intents_ca.csv\"\n",
    "\n",
    "    data = process_directory(yaml_directory)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Dataset generat amb {len(df)} frases i desat a: {output_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
