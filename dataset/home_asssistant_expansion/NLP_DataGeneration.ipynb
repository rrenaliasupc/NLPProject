{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200b2780",
   "metadata": {},
   "source": [
    "# HomeAssistant BERT Training Data generation\n",
    "\n",
    "This notebook is used to generate data to train the BERT model for using sentences in Catalan.\n",
    "\n",
    "It is based in the existing intent definition in catalan in:\n",
    "https://github.com/home-assistant/intents/tree/main/sentences/ca\n",
    "\n",
    "Data from that repository is not to train a BERT system but for using it as a phrase structure to interpret the senteces to generate intents.\n",
    "\n",
    "In this notebook, we will expand those phrases to be able to use them to train a BERT system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec80b70",
   "metadata": {},
   "source": [
    "## Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17ce78b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (6.0.1)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (2.1.4+dfsg)\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyyaml pandas --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30a0ad",
   "metadata": {},
   "source": [
    "## Import required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25768ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f75fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_expansion_rules(common_file_path):\n",
    "    \"\"\"\n",
    "    Load expansion rules from the _common.yaml file.\n",
    "    \"\"\"\n",
    "    print(f\"Loading expansion rules from {common_file_path}\")\n",
    "    with open(common_file_path, 'r', encoding='utf-8') as f:\n",
    "        content = yaml.safe_load(f)\n",
    "    return content.get('expansion_rules', {})\n",
    "\n",
    "\n",
    "def expand_rules(sentence, expansion_rules):\n",
    "    \"\"\"\n",
    "    Expand rules in the sentence using the provided expansion rules.\n",
    "    \"\"\"\n",
    "    while '<' in sentence and '>' in sentence:\n",
    "        match = re.search(r'<(.*?)>', sentence)\n",
    "        if not match:\n",
    "            break\n",
    "        rule_name = match.group(1)\n",
    "        rule_expansion = expansion_rules.get(rule_name, f\"<{rule_name}>\")\n",
    "        old_sentence = sentence\n",
    "        sentence = sentence.replace(f\"<{rule_name}>\", rule_expansion, 1)\n",
    "        if sentence == old_sentence:\n",
    "            print(f\"Warning: No expansion found for {rule_name}. Keeping original.\")\n",
    "            break\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def expand_blocks(sentence):\n",
    "    \"\"\"\n",
    "    Expand blocks in the sentence between the specified initial and end characters.\n",
    "    \"\"\"\n",
    "    initial_chars = ['(','[']\n",
    "    end_chars = [')',']']\n",
    "    expanded_sentences = []\n",
    "    expanded=False\n",
    "    \n",
    "    #if sentence contains any of the initial characters and end characters\n",
    "    if any(char in sentence for char in initial_chars) and any(char in sentence for char in end_chars):\n",
    "        end_char_pos_found= False\n",
    "        initial_char_pos2_found = False\n",
    "\n",
    "        \n",
    "        for initial_char_pos in range(len(sentence)):\n",
    "            if sentence[initial_char_pos] in initial_chars:\n",
    "                break;\n",
    "        for end_char_pos in range(initial_char_pos+1, len(sentence)):\n",
    "            if sentence[end_char_pos] in end_chars:\n",
    "                end_char_pos_found = True\n",
    "                break;\n",
    "\n",
    "        for initial_char_pos2 in range(initial_char_pos+1, len(sentence)):\n",
    "            if sentence[initial_char_pos2] in initial_chars:\n",
    "                initial_char_pos2_found = True\n",
    "                break;\n",
    "        \n",
    "        if end_char_pos_found and initial_char_pos2_found and initial_char_pos2 < end_char_pos:\n",
    "            #execute the expansion recursive between the initial2 and end characters                       \n",
    "            generatedsubstrings,expanded = expand_blocks(sentence[initial_char_pos2:end_char_pos+1])\n",
    "            for generatedsubstring in generatedsubstrings:\n",
    "                expanded_sentences.append(sentence[:initial_char_pos2] + generatedsubstring + sentence[end_char_pos+1:])\n",
    "        else:\n",
    "            #expand the sentence between the initial and end characters generate as may sentences as values separeted by |            \n",
    "            options = sentence[initial_char_pos+1:end_char_pos].split('|')            \n",
    "            for option in options:                \n",
    "                expanded_sentences.append(sentence[:initial_char_pos] + option + sentence[end_char_pos+1:])\n",
    "                expanded=True\n",
    "            #if sentence[initial_char_pos]=='[':\n",
    "            #    expanded_sentences.append(sentence[:initial_char_pos] + sentence[end_char_pos+1:])\n",
    "            #    expanded=True\n",
    "    else:\n",
    "        expanded_sentences = [sentence]\n",
    "    return expanded_sentences, expanded\n",
    "\n",
    "def expand_sentence_blocks(sentence):\n",
    "    \"\"\"\n",
    "    Expand blocks in the sentence using the provided expansion rules.\n",
    "    \"\"\"    \n",
    "    sentences= [sentence]\n",
    "    expanded=True\n",
    "    while expanded:\n",
    "        expanded=False\n",
    "        outsentences = []\n",
    "        for sentence in sentences:\n",
    "            expanded_sentences,expanded_inner=expand_blocks(sentence)\n",
    "            if expanded_inner:\n",
    "                expanded=True\n",
    "            for expanded_sentence in expanded_sentences:\n",
    "                outsentences.append(expanded_sentence)\n",
    "        #remove duplicates\n",
    "        for i in range(len(outsentences)):\n",
    "            for j in range(i+1, len(outsentences)):\n",
    "                if outsentences[i] == outsentences[j]:\n",
    "                    outsentences.pop(j)\n",
    "                    break\n",
    "        sentences = outsentences\n",
    "    return sentences\n",
    "\n",
    "def expand_sentence(sentence, expansion_rules):\n",
    "    \"\"\"\n",
    "    Expand a sentence using the provided expansion rules.\n",
    "    \"\"\"\n",
    "    sentences = [sentence]\n",
    "    outsentences=[]\n",
    "    for sentence in sentences:\n",
    "        outsentences.append(expand_rules(sentence, expansion_rules))\n",
    "\n",
    "    sentences = outsentences\n",
    "    outsentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence_outsentences = expand_sentence_blocks(sentence)\n",
    "        for sentence_outsentence in sentence_outsentences:\n",
    "            outsentences.append(sentence_outsentence)\n",
    "    sentences = outsentences\n",
    "    return sentences\n",
    "\n",
    "def load_sentences_from_yaml(file_path, expansion_rules):\n",
    "    \"\"\"\n",
    "    Load sentences from a YAML file and expand them using the provided expansion rules.\n",
    "    \"\"\"\n",
    "    print(f\"Loading sentences from {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = yaml.safe_load(f)\n",
    "    data = []\n",
    "    # Navigate through the YAML structure\n",
    "    for intent_name, intent_data in content.get('intents', {}).items():\n",
    "        for item in intent_data.get('data', []):\n",
    "            sentences = item.get('sentences', [])\n",
    "            for sentence in sentences:\n",
    "                # If slots exist in the YAML file, extract the sentence slots->domain in sentences\n",
    "                slots = item.get('slots', {})\n",
    "                domain = slots.get('domain', None)\n",
    "                if domain is None:\n",
    "                    # If no domain is found, use the intent name as the domain\n",
    "                    domain = 'None'\n",
    "                expanded = expand_sentence(sentence, expansion_rules)\n",
    "                for s in expanded:\n",
    "                    data.append({'sentence': s, 'intent': intent_name, 'domain': domain})\n",
    "    return data\n",
    "\n",
    "def process_directory(yaml_dir):\n",
    "    all_data = []\n",
    "    # Process general YAML files\n",
    "    common_file_path = os.path.join(yaml_dir, \"_common.yaml\")\n",
    "    expansion_rules = load_expansion_rules(common_file_path)\n",
    "\n",
    "    # Process each YAML file in the directory\n",
    "    for file_name in os.listdir(yaml_dir):\n",
    "        print(file_name)\n",
    "        if file_name.endswith('.yaml') or file_name.endswith('.yml'):\n",
    "            path = os.path.join(yaml_dir, file_name)\n",
    "            sentences=load_sentences_from_yaml(path,expansion_rules)\n",
    "            all_data.extend(sentences)\n",
    "            print(f\"Loaded {len(sentences)} sentences from {file_name}\")\n",
    "    return all_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d90358",
   "metadata": {},
   "source": [
    "## Process directory where intents are present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e1c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expansion rules from from_ha_intents/sentences/ca/_common.yaml\n",
      "script_HassTurnOn.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/script_HassTurnOn.yaml\n",
      "Loaded 32 sentences from script_HassTurnOn.yaml\n",
      "cover_HassSetPosition.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/cover_HassSetPosition.yaml\n",
      "Loaded 11186 sentences from cover_HassSetPosition.yaml\n",
      "light_HassTurnOff.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/light_HassTurnOff.yaml\n",
      "Warning: No expansion found for everywhere. Keeping original.\n",
      "Warning: No expansion found for everywhere. Keeping original.\n",
      "Loaded 5310 sentences from light_HassTurnOff.yaml\n",
      "cover_HassTurnOff.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/cover_HassTurnOff.yaml\n",
      "Loaded 824 sentences from cover_HassTurnOff.yaml\n",
      "vacuum_HassVacuumStart.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/vacuum_HassVacuumStart.yaml\n",
      "Loaded 48 sentences from vacuum_HassVacuumStart.yaml\n",
      "scene_HassTurnOn.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/scene_HassTurnOn.yaml\n",
      "Loaded 88 sentences from scene_HassTurnOn.yaml\n",
      "homeassistant_HassTurnOn.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/homeassistant_HassTurnOn.yaml\n",
      "Loaded 360 sentences from homeassistant_HassTurnOn.yaml\n",
      "lock_HassTurnOn.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/lock_HassTurnOn.yaml\n",
      "Loaded 352 sentences from lock_HassTurnOn.yaml\n",
      "fan_HassTurnOff.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/fan_HassTurnOff.yaml\n",
      "Loaded 320 sentences from fan_HassTurnOff.yaml\n",
      "_common.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/_common.yaml\n",
      "Loaded 0 sentences from _common.yaml\n",
      "cover_HassTurnOn.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/cover_HassTurnOn.yaml\n",
      "Loaded 1648 sentences from cover_HassTurnOn.yaml\n",
      "climate_HassTurnOff.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/climate_HassTurnOff.yaml\n",
      "Loaded 3680 sentences from climate_HassTurnOff.yaml\n",
      "cover_HassGetState.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/cover_HassGetState.yaml\n",
      "Loaded 0 sentences from cover_HassGetState.yaml\n",
      "lock_HassTurnOff.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/lock_HassTurnOff.yaml\n",
      "Loaded 960 sentences from lock_HassTurnOff.yaml\n",
      "vacuum_HassVacuumReturnToBase.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/vacuum_HassVacuumReturnToBase.yaml\n",
      "Loaded 32 sentences from vacuum_HassVacuumReturnToBase.yaml\n",
      "fan_HassTurnOn.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/fan_HassTurnOn.yaml\n",
      "Loaded 400 sentences from fan_HassTurnOn.yaml\n",
      "person_HassGetState.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/person_HassGetState.yaml\n",
      "Warning: No expansion found for on. Keeping original.\n",
      "Warning: No expansion found for on. Keeping original.\n",
      "Warning: No expansion found for on. Keeping original.\n",
      "Warning: No expansion found for la_persona. Keeping original.\n",
      "Warning: No expansion found for la_persona. Keeping original.\n",
      "Warning: No expansion found for la_persona. Keeping original.\n",
      "Loaded 16 sentences from person_HassGetState.yaml\n",
      "light_HassTurnOn.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/light_HassTurnOn.yaml\n",
      "Warning: No expansion found for everywhere. Keeping original.\n",
      "Warning: No expansion found for everywhere. Keeping original.\n",
      "Loaded 7520 sentences from light_HassTurnOn.yaml\n",
      "homeassistant_HassTurnOff.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/homeassistant_HassTurnOff.yaml\n",
      "Loaded 192 sentences from homeassistant_HassTurnOff.yaml\n",
      "climate_HassClimateSetTemperature.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/climate_HassClimateSetTemperature.yaml\n",
      "Loaded 738 sentences from climate_HassClimateSetTemperature.yaml\n",
      "climate_HassClimateGetTemperature.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/climate_HassClimateGetTemperature.yaml\n",
      "Loaded 168 sentences from climate_HassClimateGetTemperature.yaml\n",
      "climate_HassTurnOn.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/climate_HassTurnOn.yaml\n",
      "Loaded 3720 sentences from climate_HassTurnOn.yaml\n",
      "assist_satellite_HassBroadcast.yaml\n",
      "Loading sentences from from_ha_intents/sentences/ca/assist_satellite_HassBroadcast.yaml\n",
      "Loaded 6 sentences from assist_satellite_HassBroadcast.yaml\n",
      "Dataset generat amb 37600 frases i desat a: hass_intents_ca.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    yaml_directory = \"from_ha_intents/sentences/ca\"\n",
    "    output_csv = \"hass_intents_ca.csv\"\n",
    "\n",
    "    data = process_directory(yaml_directory)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Dataset generat amb {len(df)} frases i desat a: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb632c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 : executa l'script el {name}\n",
      "Type: name\n",
      "All Types: ['name', 'area', 'position', 'cover_classes:device_class', 'zone:state', 'temperature', 'message']\n"
     ]
    }
   ],
   "source": [
    "def SearchForTypes():\n",
    "    \"\"\"\n",
    "    Search for types in the generated CSV file.\n",
    "    \"\"\"\n",
    "    output_csv = \"hass_intents_ca.csv\"\n",
    "    #read the csv file\n",
    "    df = pd.read_csv(output_csv)\n",
    "    #serach for the sentences that contain { } and print number of each {types}\n",
    "    numSentence=0   \n",
    "    alltypes = []\n",
    "    for sentence in df['sentence']:    \n",
    "        numSentence+=1\n",
    "        if '{' in sentence and '}' in sentence:\n",
    "            if numSentence < 2: print(f\"Sentence {numSentence} : {sentence}\")\n",
    "            #search for the types inside the { } and print them\n",
    "            types = re.findall(r'\\{(.*?)\\}', sentence)\n",
    "            for type in types:\n",
    "                if numSentence < 2: print(f\"Type: {type}\")\n",
    "                #if the type is not in the list, add it\n",
    "                if type not in alltypes:\n",
    "                    alltypes.append(type)\n",
    "    print(f\"All Types: {alltypes}\")\n",
    "    return alltypes\n",
    "\n",
    "alltypes=SearchForTypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a02b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DefineTypesAndPossibleValues():\n",
    "\n",
    "    \"\"\"\n",
    "    Define the types and possible values for each type.\n",
    "    \"\"\"\n",
    "    # Define the types and possible values\n",
    "    alltypes= ['message', 'area', 'temperature', 'name', 'position', 'cover_classes:device_class', 'timer_seconds:start_seconds', 'timer_minutes:start_minutes', 'timer_hours:start_hours', 'timer_name:name', 'on_off_states:state', 'on_off_domains:domain', 'response', 'timer_command:conversation_command', 'timer_seconds:seconds', 'timer_minutes:minutes', 'timer_half:seconds', 'timer_hours:hours', 'timer_half:minutes', 'volume:volume_level', 'zone:state']\n",
    "    alltypespossiblevalues = {\n",
    "        'message': ['Bon dia a tothom', 'Bona tarda', 'El sopar està a punt', 'Alarma tothom a fora'],\n",
    "        'area': ['menjador', 'cuina', 'habitació', 'sala estar', 'bany', 'lavabo'],\n",
    "        'temperature': ['0','1','2','3','4','5','6','7','8','9','10',\n",
    "                        '11','12','13','14','15','16','17','18','19','20',\n",
    "                        '21','22','23','24','25','26','27','28','29','30',\n",
    "                        '31','32','33','34','35','36','37','38','39','40',\n",
    "                        '41','42','43','44','45','46','47','48','49','50'],\n",
    "        'name': ['llum cuina', 'persiana menjador', 'llum bany', 'llum habitació', 'llum sala estar', 'persiana menjador'],\n",
    "        'position': ['0','1','2','3','4','5','6','7','8','9','10',\n",
    "                        '11','12','13','14','15','16','17','18','19','20',\n",
    "                        '21','22','23','24','25','26','27','28','29','30',\n",
    "                        '31','32','33','34','35','36','37','38','39','40',\n",
    "                        '41','42','43','44','45','46','47','48','49','50',\n",
    "                        '51','52','53','54','55','56','57','58','59','60',\n",
    "                        '61','62','63','64','65','66','67','68','69','70',\n",
    "                        '71','72','73','74','75','76','77','78','79','80',\n",
    "                        '81','82','83','84','85','86','87','88','89','90',\n",
    "                        '91','92','93','94','95','96','97','98','99','100'],\n",
    "        'cover_classes:device_class': ['tendal', 'persiana', 'cortina', 'porta del garatge', 'porta', 'reixat','porticó', 'finestra'],   \n",
    "        'timer_seconds:start_seconds': ['0','1','2','3','4','5','6','7','8','9','10',\n",
    "                        '11','12','13','14','15','16','17','18','19','20',\n",
    "                        '21','22','23','24','25','26','27','28','29','30',\n",
    "                        '31','32','33','34','35','36','37','38','39','40',\n",
    "                        '41','42','43','44','45','46','47','48','49','50',\n",
    "                        '51','52','53','54','55','56','57','58','59','60'],\n",
    "        'timer_minutes:start_minutes': ['0','1','2','3','4','5','6','7','8','9','10',\n",
    "                        '11','12','13','14','15','16','17','18','19','20',\n",
    "                        '21','22','23','24','25','26','27','28','29','30',\n",
    "                        '31','32','33','34','35','36','37','38','39','40',\n",
    "                        '41','42','43','44','45','46','47','48','49','50',\n",
    "                        '51','52','53','54','55','56','57','58','59','60'],\n",
    "        'timer_hours:start_hours': ['0','1','2','3','4','5','6','7','8','9','10',\n",
    "                        '11','12','13','14','15','16','17','18','19','20',\n",
    "                        '21','22','23','24','25','26','27','28','29','30',\n",
    "                        '31','32','33','34','35','36','37','38','39','40',\n",
    "                        '41','42','43','44','45','46','47','48','49','50',\n",
    "                        '51','52','53','54','55','56','57','58','59','60'],\n",
    "        'timer_name:name': ['temps cuina', 'temps espera'],\n",
    "        'on_off_states:state': ['engegat', 'ences', 'aturat', 'apagat', 'desconnectat'],\n",
    "        'on_off_domains:domain': ['llum', 'ventilador','interruptor','pany'],\n",
    "        'response': ['Sí', 'No', 'fet', 'no ho sé'],\n",
    "        'timer_command:conversation_command': ['encendre', 'apagar'],\n",
    "        'timer_seconds:seconds': ['0','1','2','3','4','5','6','7','8','9','10',\n",
    "                        '11','12','13','14','15','16','17','18','19','20',\n",
    "                        '21','22','23','24','25','26','27','28','29','30',\n",
    "                        '31','32','33','34','35','36','37','38','39','40',\n",
    "                        '41','42','43','44','45','46','47','48','49','50',\n",
    "                        '51','52','53','54','55','56','57','58','59','60'],\n",
    "        'timer_minutes:minutes': ['0','1','2','3','4','5','6','7','8','9','10',\n",
    "                        '11','12','13','14','15','16','17','18','19','20',\n",
    "                        '21','22','23','24','25','26','27','28','29','30',\n",
    "                        '31','32','33','34','35','36','37','38','39','40',\n",
    "                        '41','42','43','44','45','46','47','48','49','50',\n",
    "                        '51','52','53','54','55','56','57','58','59','60'],\n",
    "        'timer_half:seconds': ['0','1','2','3','4','5','6','7','8','9','10',\n",
    "                        '11','12','13','14','15','16','17','18','19','20',\n",
    "                        '21','22','23','24','25','26','27','28','29','30',\n",
    "                        '31','32','33','34','35','36','37','38','39','40',\n",
    "                        '41','42','43','44','45','46','47','48','49','50',\n",
    "                        '51','52','53','54','55','56','57','58','59','60'],\n",
    "        'timer_hours:hours': ['0','1','2','3','4','5','6','7','8','9','10',\n",
    "                        '11','12','13','14','15','16','17','18','19','20',\n",
    "                        '21','22','23','24','25','26','27','28','29','30',\n",
    "                        '31','32','33','34','35','36','37','38','39','40',\n",
    "                        '41','42','43','44','45','46','47','48','49','50',\n",
    "                        '51','52','53','54','55','56','57','58','59','60'],\n",
    "        'timer_half:minutes': ['0','1','2','3','4','5','6','7','8','9','10',\n",
    "                        '11','12','13','14','15','16','17','18','19','20',\n",
    "                        '21','22','23','24','25','26','27','28','29','30',\n",
    "                        '31','32','33','34','35','36','37','38','39','40',\n",
    "                        '41','42','43','44','45','46','47','48','49','50',\n",
    "                        '51','52','53','54','55','56','57','58','59','60'],\n",
    "        'volume:volume_level': ['0','1','2','3','4','5','6','7','8','9','10',\n",
    "                        '11','12','13','14','15','16','17','18','19','20',\n",
    "                        '21','22','23','24','25','26','27','28','29','30',\n",
    "                        '31','32','33','34','35','36','37','38','39','40',\n",
    "                        '41','42','43','44','45','46','47','48','49','50',\n",
    "                        '51','52','53','54','55','56','57','58','59','60',\n",
    "                        '61','62','63','64','65','66','67','68','69','70',\n",
    "                        '71','72','73','74','75','76','77','78','79','80',\n",
    "                        '81','82','83','84','85','86','87','88','89','90',\n",
    "                        '91','92','93','94','95','96','97','98','99','100'],\n",
    "        'zone:state': ['menjador', 'cuina', 'habitació', 'sala estar', 'bany', 'lavabo'],\n",
    "        # Add more types and possible values as needed\n",
    "    }\n",
    "    return alltypes, alltypespossiblevalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94cc895e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Types: ['message', 'area', 'temperature', 'name', 'position', 'cover_classes:device_class', 'timer_seconds:start_seconds', 'timer_minutes:start_minutes', 'timer_hours:start_hours', 'timer_name:name', 'on_off_states:state', 'on_off_domains:domain', 'response', 'timer_command:conversation_command', 'timer_seconds:seconds', 'timer_minutes:minutes', 'timer_half:seconds', 'timer_hours:hours', 'timer_half:minutes', 'volume:volume_level', 'zone:state']\n",
      "All Types Possible Values: {'message': ['Bon dia a tothom', 'Bona tarda', 'El sopar està a punt', 'Alarma tothom a fora'], 'area': ['menjador', 'cuina', 'habitació', 'sala estar', 'bany', 'lavabo'], 'temperature': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50'], 'name': ['llum cuina', 'persiana menjador', 'llum bany', 'llum habitació', 'llum sala estar', 'persiana menjador'], 'position': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100'], 'cover_classes:device_class': ['tendal', 'persiana', 'cortina', 'porta del garatge', 'porta', 'reixat', 'porticó', 'finestra'], 'timer_seconds:start_seconds': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60'], 'timer_minutes:start_minutes': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60'], 'timer_hours:start_hours': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60'], 'timer_name:name': ['temps cuina', 'temps espera'], 'on_off_states:state': ['engegat', 'ences', 'aturat', 'apagat', 'desconnectat'], 'on_off_domains:domain': ['llum', 'ventilador', 'interruptor', 'pany'], 'response': ['Sí', 'No', 'fet', 'no ho sé'], 'timer_command:conversation_command': ['encendre', 'apagar'], 'timer_seconds:seconds': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60'], 'timer_minutes:minutes': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60'], 'timer_half:seconds': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60'], 'timer_hours:hours': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60'], 'timer_half:minutes': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60'], 'volume:volume_level': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100'], 'zone:state': ['menjador', 'cuina', 'habitació', 'sala estar', 'bany', 'lavabo']}\n",
      "All Labels: ['B-message', 'I-message', 'B-area', 'I-area', 'B-temperature', 'I-temperature', 'B-name', 'I-name', 'B-position', 'I-position', 'B-cover_classes:device_class', 'I-cover_classes:device_class', 'B-timer_seconds:start_seconds', 'I-timer_seconds:start_seconds', 'B-timer_minutes:start_minutes', 'I-timer_minutes:start_minutes', 'B-timer_hours:start_hours', 'I-timer_hours:start_hours', 'B-timer_name:name', 'I-timer_name:name', 'B-on_off_states:state', 'I-on_off_states:state', 'B-on_off_domains:domain', 'I-on_off_domains:domain', 'B-response', 'I-response', 'B-timer_command:conversation_command', 'I-timer_command:conversation_command', 'B-timer_seconds:seconds', 'I-timer_seconds:seconds', 'B-timer_minutes:minutes', 'I-timer_minutes:minutes', 'B-timer_half:seconds', 'I-timer_half:seconds', 'B-timer_hours:hours', 'I-timer_hours:hours', 'B-timer_half:minutes', 'I-timer_half:minutes', 'B-volume:volume_level', 'I-volume:volume_level', 'B-zone:state', 'I-zone:state', 'O']\n",
      "All Intents: ['HassTurnOn', 'HassSetPosition', 'HassTurnOff', 'HassVacuumStart', 'HassVacuumReturnToBase', 'HassGetState', 'HassClimateSetTemperature', 'HassClimateGetTemperature', 'HassBroadcast', 'none']\n",
      "All Domains: ['script', 'cover', 'light', nan, 'scene', 'lock', 'fan', 'climate', 'person', 'none']\n"
     ]
    }
   ],
   "source": [
    "def SearchForLabels():\n",
    "    \"\"\"Search for types in the generated CSV file.\"\"\"\n",
    "    alltypes, alltypespossiblevalues = DefineTypesAndPossibleValues()\n",
    "    print(f\"All Types: {alltypes}\")\n",
    "    print(f\"All Types Possible Values: {alltypespossiblevalues}\")\n",
    "\n",
    "    # Define all possible labels based on the types\n",
    "    all_labels = []\n",
    "    for type in alltypes:\n",
    "        all_labels.append(\"B-\" + type)\n",
    "        all_labels.append(\"I-\" + type)\n",
    "    all_labels.append(\"O\")\n",
    "\n",
    "    print(f\"All Labels: {all_labels}\")\n",
    "    #save the labels to json file\n",
    "    with open('slots.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_labels, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    output_csv = \"hass_intents_ca.csv\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(output_csv)\n",
    "    numSentence = 0\n",
    "\n",
    "    # Create a DataFrame with the columns sentence, text, intent, and labels\n",
    "    df_labels = pd.DataFrame(columns=['sentence', 'text', 'intent', 'labels'])\n",
    "\n",
    "    # Extract sentences and intents from the CSV file\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        intent = row['intent']\n",
    "        domain = row['domain']\n",
    "        if intent not in [\"HassTurnOn\", \"HassTurnOff\"]: continue\n",
    "\n",
    "        numSentence += 1\n",
    "        words = sentence.split(' ')\n",
    "        labels = []\n",
    "        new_sentence = \"\"\n",
    "\n",
    "        for word in words:\n",
    "            if '{' in word and '}' in word:\n",
    "                types = re.findall(r'\\{(.*?)\\}', word)\n",
    "                for type in types:\n",
    "                    if type in alltypes:\n",
    "                        substext = random.choice(alltypespossiblevalues[type])\n",
    "                        #print(f\"Type: {type}, Subtext: {substext}\")\n",
    "                        subtextwords = substext.split(' ')\n",
    "                        first = True\n",
    "                        for subtextword in subtextwords:\n",
    "                            if first:\n",
    "                                labels.append(\"B-\" + type)\n",
    "                                first = False\n",
    "                            else:\n",
    "                                labels.append(\"I-\" + type)\n",
    "                            new_sentence += subtextword + \" \"\n",
    "                    else:\n",
    "                        labels.append(\"O\")\n",
    "                        new_sentence += word + \" \"\n",
    "            else:\n",
    "                labels.append(\"O\")\n",
    "                new_sentence += word + \" \"\n",
    "        new_sentence = new_sentence[:-1]\n",
    "\n",
    "        # Add the new row to the DataFrame\n",
    "        new_row = {'sentence': sentence, 'text': new_sentence, 'intent': intent, 'domain': domain, 'labels': labels}\n",
    "        df_labels = pd.concat([df_labels, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        #print(f\"Sentence {numSentence} : {sentence}, NewSentence: {new_sentence}, Intent: {intent}, Labels: {labels}\")\n",
    "\n",
    "    # Save the DataFrame to a JSON file\n",
    "    # Converteix el DataFrame a una llista de diccionaris\n",
    "    data = df_labels.to_dict(orient='records')\n",
    "\n",
    "    # Escriu el fitxer JSON amb ensure_ascii=False\n",
    "    with open('dataset_homeassistant.jsonl', 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join([json.dumps(c, ensure_ascii=False) for c in data]) + '\\n')\n",
    "\n",
    "    # Extract all the intents from the CSV file\n",
    "    all_intents = df['intent'].unique()\n",
    "    # Add intent \"none\" to the list of intents\n",
    "    all_intents = list(all_intents) + ['none']\n",
    "\n",
    "    \n",
    "    print(f\"All Intents: {all_intents}\")\n",
    "    # Save the intents to a JSON file\n",
    "    with open('intent-types.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_intents, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Extract all the domains from the CSV file\n",
    "    all_domains = df['domain'].unique()\n",
    "    # Add domain \"none\" to the list of domains\n",
    "    all_domains = list(all_domains) + ['none']\n",
    "    print(f\"All Domains: {all_domains}\")\n",
    "    # Save the domains to a JSON file\n",
    "    with open('domain-types.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_domains, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        \n",
    "\n",
    "SearchForLabels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "497552c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with LLM dataset\n",
    "\n",
    "with open('dataset_homeassistant.jsonl', 'r', encoding='utf-8') as f1:\n",
    "    with open('../dataset_augmented.jsonl', 'r', encoding='utf-8') as f2:\n",
    "        with open('../dataset_augmented_merged.jsonl', 'w', encoding='utf-8') as f3:\n",
    "            f3.write(f1.read() + f2.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e1881",
   "metadata": {},
   "source": [
    "## Notes Ricard\n",
    "\n",
    "Faltaria polir del dataset:\n",
    "\n",
    "* Posar B-ACTION-TURN-ON o B-ACTION-TURN-OFF a la primera paraula de cada frase excepte:\n",
    "  * Si comença la frase amb pots/podries, llavors és la segona paraula.\n",
    "  * Si comença amb \"tornar a\" és la tercera paraula.\n",
    "* Hi ha moltes frases sense cap etiqueta (les que contenen \"llum\"). S'hauria de, si no hi ha cap etiqueta, buscar \"llum\" i posar-li manualment l'etiqueta.\n",
    "* Hi ha moltes fraess amb \"<everywhere>\". S'hauria de treure i posar etiqueta b-place (o la que toqui) a les 3 últimes paraules (\"A tot arreu\").\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
